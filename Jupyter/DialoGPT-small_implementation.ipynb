{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8de5810-6d7f-45b0-a722-fef3a6ec9aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d62a8750-6b51-4b1d-8f0b-79996367f907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62a05d50-f2a5-4cdb-ad01-9d92f76ebd69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogpt_name = \"microsoft/DialoGPT-small\"\n",
    "\n",
    "dialogpt_tokenizer = GPT2Tokenizer.from_pretrained(dialogpt_name)\n",
    "dialogpt_model = GPT2LMHeadModel.from_pretrained(dialogpt_name)\n",
    "\n",
    "# Same fix applies\n",
    "dialogpt_tokenizer.pad_token = dialogpt_tokenizer.eos_token\n",
    "dialogpt_conversation_ids = None\n",
    "\n",
    "\n",
    "dialogpt_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da069a7b-2b5a-4854-84e0-bba17da894be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_dialogpt(user_input, max_new_tokens=80):\n",
    "    global dialogpt_conversation_ids\n",
    "\n",
    "    # Encode user input + EOS\n",
    "    new_input_ids = dialogpt_tokenizer.encode(\n",
    "        user_input + dialogpt_tokenizer.eos_token,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Append conversation history\n",
    "    if dialogpt_conversation_ids is None:\n",
    "        bot_input_ids = new_input_ids\n",
    "    else:\n",
    "        bot_input_ids = torch.cat(\n",
    "            [dialogpt_conversation_ids, new_input_ids],\n",
    "            dim=-1\n",
    "        )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        dialogpt_conversation_ids = dialogpt_model.generate(\n",
    "            bot_input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.2,\n",
    "            pad_token_id=dialogpt_tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Decode only newly generated tokens\n",
    "    reply = dialogpt_tokenizer.decode(\n",
    "        dialogpt_conversation_ids[:, bot_input_ids.shape[-1]:][0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    return reply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bbc759-b8c0-4df7-8c18-42a3709f102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "\n",
    "    print(\"DialoGPT:\", chat_dialogpt(user_input))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b14bcd-3d44-4b34-9242-9b747e4ccfaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
