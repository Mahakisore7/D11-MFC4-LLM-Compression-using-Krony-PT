{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68f58eee-5b12-4e33-bd95-aabe133730ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (4.57.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from torch) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from torch) (2026.1.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from transformers) (2026.1.15)\n",
      "Requirement already satisfied: requests in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from requests->transformers) (2.6.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from requests->transformers) (2026.1.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9156e3f8-d81f-4e9f-addc-7e6f38cc6f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3262562-ba59-418e-bb44-ddcba2b4c8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Tokenizer...\n",
      "Loading Model...\n",
      "Success! GPT-2 is loaded.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2\" \n",
    "print(\"Loading Tokenizer...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "print(\"Loading Model...\")\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model = model.to(device) \n",
    "model.eval()\n",
    "print(\"Success! GPT-2 is loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0da9190-6b51-479a-9450-5782854d151d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# ARCHITECTURE\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a3d9609-578a-4c61-bc43-2824202e4d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tokens: tensor([[  464,  2068,  7586, 21831]])\n",
      "\n",
      "Output: The quick brown foxes are a great way to get a little bit of a kick out of your\n"
     ]
    }
   ],
   "source": [
    "# 5. TEST GENERATION\n",
    "input_text = \"The quick brown fox\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "print(f\"Input Tokens: {inputs['input_ids']}\")\n",
    "\n",
    "# Ask the model to continue the sentence\n",
    "with torch.no_grad(): # Disable gradient calculation to save memory\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_length=20, \n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "decoded_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"\\nOutput:\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0c48603-d2f8-47a7-939e-a556adcbfb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "   BASELINE GPT-2 (UNCOMPRESSED)\n",
      "   (Type 'exit' to stop)\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "YOU:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:  I am not sure if you know what a \"Hammer\" means, but it's probably an English word for something that was written in England and then translated into French by someone else who did so too (or maybe they were both writers). It\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "YOU:  What is Artificial Inteliigence ? \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:  It's a computer program that can read and write to any number, including numbers with no digits or letters in them (like \"1\" for example). This means it has an infinite amount more memory than most computers on earth! You could say this\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "YOU:  2+2 = ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:  Yes, it's a question that has been asked many times before and I think we all know what to do with this one!The following article was originally published on May 18th 2015 at 9am by David Boulton in his blog \"\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "YOU:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending chat...\n"
     ]
    }
   ],
   "source": [
    "# BLOCK 5: ROBUST CHAT (WITH REPETITION PENALTY)\n",
    "# This version fixes the \"AI is AI\" loop problem.\n",
    "\n",
    "def chat_with_model_robust():\n",
    "    print(\"=\"*50)\n",
    "    print(\"   BASELINE GPT-2 (UNCOMPRESSED)\")\n",
    "    print(\"   (Type 'exit' to stop)\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # STRICTER PRIMER: Forces Q&A format\n",
    "    primer = \"\"\"\n",
    "Q: What is the capital of France?\n",
    "A: The capital of France is Paris.\n",
    "\n",
    "Q: What is 2 + 2?\n",
    "A: The answer is 4.\n",
    "\n",
    "Q: Who wrote Hamlet?\n",
    "A: William Shakespeare.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"YOU: \")\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"Ending chat...\")\n",
    "            break\n",
    "        \n",
    "        # Format input to match the primer\n",
    "        full_prompt = f\"{primer}Q: {user_input}\\nA:\"\n",
    "        \n",
    "        inputs = tokenizer(full_prompt, return_tensors=\"pt\", padding=True).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                inputs.input_ids,\n",
    "                attention_mask=inputs.attention_mask,\n",
    "                max_new_tokens=50,\n",
    "                do_sample=True,\n",
    "                top_k=50,\n",
    "                top_p=0.90,             # Stricter (was 0.95)\n",
    "                temperature=0.1,        # Lower temp = Less random/crazy\n",
    "                repetition_penalty=1.2, # <--- THIS FIXES THE LOOPING\n",
    "                no_repeat_ngram_size=2, # <--- PREVENTS \"AI is AI\" phrases\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.encode(\"\\n\")[0]\n",
    "            )\n",
    "\n",
    "        full_response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract answer\n",
    "        # We split by \"A:\" and take the last part\n",
    "        try:\n",
    "            # Find the part after our specific prompt\n",
    "            new_text = full_response[len(full_prompt):]\n",
    "            # Stop at the next new line (if the model tries to write a new Q:)\n",
    "            answer = new_text.split(\"\\n\")[0].strip()\n",
    "            # If empty (sometimes happens), take the raw generation\n",
    "            if not answer: \n",
    "                answer = new_text.strip()\n",
    "        except:\n",
    "            answer = \"...\"\n",
    "\n",
    "        print(f\"AI:  {answer}\\n\")\n",
    "\n",
    "# Run the Fixed Chat\n",
    "chat_with_model_robust()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9b0ca1-941d-4b40-81f9-9d3f7f78c1c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
