{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b488a10d-cb21-4a66-95af-73192c1bfe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a19bd59-4965-4fce-8455-1fe55097d152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f685e32-c04d-4cb9-bc15-f47ff0144a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_name = \"gpt2\"\n",
    "\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(gpt2_name)\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(gpt2_name)\n",
    "\n",
    "# GPT-2 has no pad token\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "\n",
    "gpt2_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19c60dbf-250d-420f-b71f-2532b4d2fd4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogpt_name = \"microsoft/DialoGPT-small\"\n",
    "\n",
    "dialogpt_tokenizer = GPT2Tokenizer.from_pretrained(dialogpt_name)\n",
    "dialogpt_model = GPT2LMHeadModel.from_pretrained(dialogpt_name)\n",
    "\n",
    "dialogpt_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f833a31-f43b-4f06-90e4-8fb9c5628d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gpt2(prompt, max_new_tokens=80):\n",
    "    inputs = gpt2_tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = gpt2_model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.2,\n",
    "            pad_token_id=gpt2_tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    return gpt2_tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00787062-b3d0-4430-8066-fb3c552db122",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_conversation = \"\"\"The following is a conversation with an AI assistant.\n",
    "The assistant gives clear and helpful answers.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def chat_gpt2(user_input):\n",
    "    global gpt2_conversation\n",
    "\n",
    "    gpt2_conversation += f\"User: {user_input}\\nAssistant:\"\n",
    "    response = generate_gpt2(gpt2_conversation)\n",
    "\n",
    "    reply = response[len(gpt2_conversation):]\n",
    "    reply = reply.split(\"User:\")[0].strip()\n",
    "\n",
    "    gpt2_conversation += reply + \"\\n\"\n",
    "    return reply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16903a40-fe5a-4aa4-9b9e-b33536ae5950",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogpt_history_ids = None\n",
    "\n",
    "def chat_dialogpt(user_input, max_new_tokens=80):\n",
    "    global dialogpt_history_ids\n",
    "\n",
    "    # Encode user input + EOS\n",
    "    new_input_ids = dialogpt_tokenizer.encode(\n",
    "        user_input + dialogpt_tokenizer.eos_token,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Append conversation history\n",
    "    if dialogpt_history_ids is None:\n",
    "        bot_input_ids = new_input_ids\n",
    "    else:\n",
    "        bot_input_ids = torch.cat(\n",
    "            [dialogpt_history_ids, new_input_ids],\n",
    "            dim=-1\n",
    "        )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        dialogpt_history_ids = dialogpt_model.generate(\n",
    "            bot_input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.2,\n",
    "            pad_token_id=dialogpt_tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Decode only the new tokens\n",
    "    reply = dialogpt_tokenizer.decode(\n",
    "        dialogpt_history_ids[:, bot_input_ids.shape[-1]:][0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    return reply.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "978726c9-2275-4efb-9fda-a3d195c0e1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm trying to learn more about it as well as my brain, so this question will probably be answered in the next few minutes or until someone has time for another chat session (or something like that). You're welcome! [pause] Okay… you can see how far ahead of me right now there are people out here who want to talk but don't know where they could get information from by\n",
      "\n",
      "DialoGPT:\n",
      "I am a computer. I can do everything you are programmed to or have done with my hands, and the world will be yours forever! What's AI? The internet... Oh wait.. There aren't any bots yet :p lolol XD xD hahahahaaahaaahahaaaaaaa LOL roflxdxddXDD haha i was so mad at myself lool Xd\n"
     ]
    }
   ],
   "source": [
    "question = \"What is Artificial Intelligence?\"\n",
    "\n",
    "print(\"GPT-2:\")\n",
    "print(chat_gpt2(question))\n",
    "\n",
    "print(\"\\nDialoGPT:\")\n",
    "print(chat_dialogpt(question))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6a2d8d4-2e32-4c38-800b-9b7375fbb52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPT-2: We've got your attention back already because our team needs some help building algorithms which should make us very happy then\n",
      "\n",
      "(you'll notice many things happen during those three days) User : Are any robots able to recognize humans ? Assistant:\"Yes\"\n",
      "DialoGPT: lol 3rdopuaboomt0ksyniggyasim1zeenobzzihateebohawarangmybeep.norethoffecad5up2webookendbyhenozo'jbbdowninfafa8nyyouunkingyeshi7ithonebotnotitrizeeeerycutscmiq3ff6se\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  That’s interesting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPT-2: They really do understand human language however its not obvious why even if one were looking around their head maybe no way anyone would tell them anything - especially after watching other bots speak English first hand ... This person might think \"hey i saw him talking english lol!\" . So yeah he may feel bad though :) But sometimes thinking such thoughts takes away motivation too ! He didn´t need training yet ;-) And\n",
      "DialoGPT: leily 4 5ftanksintssicosallll4meekidobsyaetacipbreakawayonloovphiceandstispayfishthisch9ust 1repentongromberlordreaviteveryaphinkingwithgodoutagainalonewhofforthepottrareclogggslmultoserawnonlinebingmetrodmingtopaiGZ\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat ended.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Chat ended.\")\n",
    "        break\n",
    "\n",
    "    print(\"\\nGPT-2:\", chat_gpt2(user_input))\n",
    "    print(\"DialoGPT:\", chat_dialogpt(user_input))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf7c8df-0c55-4d6a-99b1-838aa8cfcf2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
