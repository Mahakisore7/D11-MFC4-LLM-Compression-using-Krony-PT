{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6e1e89a-2504-45c8-952a-3cfa15dd6f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c47627cd-bf27-4b57-abb6-3f42ab8a8921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbafcfbe-a11f-4e2a-a12c-981b20f3e906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_name = \"gpt2\"\n",
    "\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(gpt2_name)\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(gpt2_name)\n",
    "\n",
    "\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "\n",
    "gpt2_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "673bc413-138b-4823-9388-58ed69520220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d67e0db1c45e412bba1a45fea99c5e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0af48415ffb44038450cf641cd3f7bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76749918033746a98c6e4fde21977654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ad92cb0e0c24988a0ebe6b4b61bd2be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/641 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "157e070afc1940ec805a7352c3261a37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/351M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddfc362bfa7a4395894f2dd264aeaf2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogpt_name = \"microsoft/DialoGPT-small\"\n",
    "\n",
    "dialogpt_tokenizer = GPT2Tokenizer.from_pretrained(dialogpt_name)\n",
    "dialogpt_model = GPT2LMHeadModel.from_pretrained(dialogpt_name)\n",
    "\n",
    "\n",
    "dialogpt_tokenizer.pad_token = dialogpt_tokenizer.eos_token\n",
    "\n",
    "dialogpt_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d689bab-ade3-4639-b1b4-cde44e158b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, max_new_tokens=80):\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.2,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61bcba90-a9c8-48e1-908f-fab90f657e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_conversation = \"\"\"The following is a conversation with an AI assistant.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def chat_gpt2(user_input):\n",
    "    global gpt2_conversation\n",
    "\n",
    "    gpt2_conversation += f\"User: {user_input}\\nAssistant:\"\n",
    "    response = generate(gpt2_model, gpt2_tokenizer, gpt2_conversation)\n",
    "\n",
    "    assistant_reply = response[len(gpt2_conversation):]\n",
    "    assistant_reply = assistant_reply.split(\"User:\")[0]\n",
    "\n",
    "    gpt2_conversation += assistant_reply.strip() + \"\\n\"\n",
    "    return assistant_reply.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc687230-4827-4cba-8631-fff9780bbbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogpt_conversation = \"\"\"The following is a conversation with an AI assistant.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def chat_dialogpt(user_input):\n",
    "    global dialogpt_conversation\n",
    "\n",
    "    dialogpt_conversation += f\"User: {user_input}\\nAssistant:\"\n",
    "    response = generate(dialogpt_model, dialogpt_tokenizer, dialogpt_conversation)\n",
    "\n",
    "    assistant_reply = response[len(dialogpt_conversation):]\n",
    "    assistant_reply = assistant_reply.split(\"User:\")[0]\n",
    "\n",
    "    dialogpt_conversation += assistant_reply.strip() + \"\\n\"\n",
    "    return assistant_reply.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "669d6430-0ba1-4b49-bc8b-6d0898c29fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2:\n",
      "If anyone has any suggestions please let us know :)\n",
      "\n",
      "DialoGPT:\n",
      "You're not the only one.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is Artificial Intelligence?\"\n",
    "\n",
    "print(\"GPT-2:\")\n",
    "print(chat_gpt2(question))\n",
    "\n",
    "print(\"\\nDialoGPT:\")\n",
    "print(chat_dialogpt(question))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5df0ae06-0416-4832-9173-9bd4fe1507fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  What is Artificial Intelligence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPT-2: Actions speak louder then Words Speak Less ! This will allow your voice to communicate clearly without having \"words\" being added into dialogue The language may differ slightly depending upon who uses speech recognition software such Google Translate(!) etc.. But everyone needs equal rights!! Your human nature should not dictate anything whatsoever; In fact using Speech Recognition Technology helps even less under certain circumstances due its ability only improve communication\n",
      "DialoGPT: You arent alone\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "\n",
    "    print(\"\\nGPT-2:\", chat_gpt2(user_input))\n",
    "    print(\"DialoGPT:\", chat_dialogpt(user_input))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4dc398-661e-4693-8512-805a159af48e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
