{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d988c503-73a8-48ef-b35f-2fbf543202bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (4.57.6)\n",
      "Requirement already satisfied: torch in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from transformers) (3.20.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from transformers) (2.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from transformers) (2026.1.15)\n",
      "Requirement already satisfied: requests in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2026.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from requests->transformers) (2.6.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mahakisore\\miniconda3\\lib\\site-packages (from requests->transformers) (2026.1.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4606f656-f9ab-4ff6-bb01-36b890fd30c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d48de9ea-c5d3-48bd-8e17-cb3f909a9d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"gpt2\"   # this IS GPT-2 small\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "model.eval()  # put model in inference mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b380370-af33-4463-af88-1416b26546e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial intelligence is a very powerful tool that could be used to solve many problems. One of the main reasons for the importance of artificial intelligence is because it is able to solve complex problems such as classification, error analysis or statistical analysis.\n",
      "\n",
      "In\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Artificial intelligence is\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=50,\n",
    "        do_sample=True,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f6bccf3-e45c-458a-9299-6e68f8d45d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, max_length=150):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            temperature=0.8,\n",
    "            top_p=0.9\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb55f267-c316-4ddc-81b9-7b86c924c148",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hello\n",
      "Assistant: Hello\n",
      "Assistant: Hello\n",
      "Assistant: Hello\n",
      "Assistant: Hello\n",
      "Assistant: Hello\n",
      "Assistant: Hello\n",
      "Assistant: Hello\n",
      "Assistant: Hello\n",
      "Assistant: Hello\n",
      "Assistant: Hello\n",
      "Assistant: Hello\n",
      "Assistant: Hello\n",
      "Assistant: Hello\n",
      "Assistant: Hello\n",
      "Assistant: Hello\n",
      "Assistant: Hello\n",
      "Assistant: Hello\n",
      "Assistant: Hello\n",
      "Assistant: Hello\n",
      "Assistant: Hello\n",
      "Assistant: Hello\n",
      "Assistant: Hello\n",
      "Assistant: Hello\n",
      "Assistant: Hello\n",
      "Assistant: Hello\n",
      "Assistant: Hello\n",
      "Assistant: Hello\n",
      "Assistant: Hello\n",
      "Assistant: Hello\n",
      "Assistant: Hello\n",
      "Assistant: Hello\n",
      "Assistant: Hello\n",
      "Assistant: Hello\n",
      "Assistant: Hello\n",
      "Assistant: Hello\n",
      "Assistant: Hello\n",
      "Assistant:\n"
     ]
    }
   ],
   "source": [
    "chat_prompt = \"\"\"User: Hello\n",
    "Assistant:\"\"\"\n",
    "\n",
    "response = generate_response(chat_prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4c5f027-a131-4ba5-870e-18f6a36aae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = \"\"\n",
    "\n",
    "def chat(user_input):\n",
    "    global conversation\n",
    "\n",
    "    conversation += f\"User: {user_input}\\nAssistant:\"\n",
    "    response = generate_response(conversation)\n",
    "\n",
    "    # Extract only the last assistant reply\n",
    "    assistant_reply = response[len(conversation):].split(\"User:\")[0]\n",
    "\n",
    "    conversation += assistant_reply + \"\\n\"\n",
    "    return assistant_reply.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c3c7cb1-2484-4239-84cb-f49032407e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  What is Artificial Intelligence ? \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2: What is Artificial Intelligence ?\n",
      "Assistant: Where are we going ? \n",
      "Assistant: Are we going to use it for anything ? \n",
      "Assistant: What is it ? \n",
      "Assistant: We have no choice? \n",
      "Assistant: No. \n",
      "Assistant: I'm not going to let you down. \n",
      "Assistant: I can't. \n",
      "Assistant: That's not what you're talking about. \n",
      "Assistant: It's not going to work. \n",
      "Assistant: Well, what you're saying is, if we're going to do that, we're going to do it for you. \n",
      "Assistant: I mean, that's right. \n",
      "Assistant: Yeah\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "\n",
    "    reply = chat(user_input)\n",
    "    print(\"GPT-2:\", reply)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e22ca8f-03df-40dc-a693-29f8fe4124a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
